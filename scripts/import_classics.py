#!/usr/bin/env python3
"""
ä¸­å›½å¤å…¸æ–‡çŒ®æ•°å­—åŒ–èµ„æ–™å¯¼å…¥Elasticsearch
ä¸“é—¨å¤„ç†daizhigev20ç›®å½•ä¸‹çš„ä¼ ç»Ÿå…¸ç±æ•°æ®
"""

import os
import json
import hashlib
import re
from pathlib import Path
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import argparse
from datetime import datetime

# å…¸ç±åˆ†ç±»æ˜ å°„
COLLECTION_MAPPING = {
    'ä½›è—': 'Buddhist Texts',
    'å„’è—': 'Confucian Classics', 
    'åŒ»è—': 'Medical Texts',
    'å²è—': 'Historical Records',
    'å­è—': 'Masters Literature',
    'æ˜“è—': 'I Ching Studies',
    'è‰ºè—': 'Arts & Crafts',
    'è¯—è—': 'Poetry Collection',
    'é“è—': 'Taoist Texts',
    'é›†è—': 'Collected Works'
}

def connect_to_elasticsearch():
    """è¿æ¥åˆ°Elasticsearch"""
    try:
        es = Elasticsearch([{'host': 'localhost', 'port': 9200}])
        if es.ping():
            print("âœ… æˆåŠŸè¿æ¥åˆ°Elasticsearch")
            cluster_info = es.info()
            print(f"é›†ç¾¤ä¿¡æ¯: {cluster_info['cluster_name']} - ç‰ˆæœ¬: {cluster_info['version']['number']}")
            return es
        else:
            print("âŒ æ— æ³•è¿æ¥åˆ°Elasticsearch")
            return None
    except Exception as e:
        print(f"âŒ è¿æ¥é”™è¯¯: {e}")
        return None

def extract_text_metadata(content, filepath):
    """ä»æ–‡æœ¬å†…å®¹ä¸­æå–å…ƒæ•°æ®"""
    metadata = {}
    
    # å°è¯•æå–ä¹¦åï¼ˆé€šå¸¸åœ¨å¼€å¤´å‡ è¡Œï¼‰
    lines = content.split('\n')[:20]
    for line in lines:
        line = line.strip()
        if line and len(line) < 50:
            # å¯èƒ½æ˜¯ä¹¦åæˆ–ç« èŠ‚å
            if any(char in line for char in ['å·', 'ç¯‡', 'ç« ', 'ç¬¬']):
                metadata['chapter'] = line
                break
            elif len(line) < 20 and not any(char in line for char in ['ã€‚', 'ï¼Œ', 'ã€']):
                metadata['title'] = line
                break
    
    # ä»æ–‡ä»¶è·¯å¾„æå–ä¹¦ç±ä¿¡æ¯
    path_parts = filepath.split(os.sep)
    if len(path_parts) >= 3:
        # æ‰¾åˆ°è—çš„åç§°
        collection = 'æœªçŸ¥'
        book_category = ''
        for i, part in enumerate(path_parts):
            if part in COLLECTION_MAPPING:
                collection = part
                if i + 1 < len(path_parts):
                    book_category = path_parts[i + 1]
                break
        
        filename = path_parts[-1]
        
        metadata['collection'] = collection
        metadata['collection_en'] = COLLECTION_MAPPING.get(collection, collection)
        metadata['book_category'] = book_category
        metadata['filename'] = filename
    
    # ä¼°ç®—æ–‡æœ¬ç‰¹å¾
    metadata['char_count'] = len(content)
    metadata['line_count'] = len(content.split('\n'))
    
    # æ£€æµ‹æ˜¯å¦åŒ…å«å¤æ–‡ç‰¹å¾
    classical_indicators = ['æ›°', 'è€…', 'ä¹Ÿ', 'çŸ£', 'ç„‰', 'ä¹', 'å“‰', 'è€¶']
    classical_score = sum(content.count(char) for char in classical_indicators)
    metadata['classical_score'] = classical_score
    metadata['is_classical'] = classical_score > 10
    
    return metadata

def process_text_file(filepath, base_dir):
    """å¤„ç†å•ä¸ªå¤å…¸æ–‡çŒ®æ–‡ä»¶"""
    try:
        # è·å–ç›¸å¯¹è·¯å¾„
        relative_path = os.path.relpath(filepath, base_dir)
        
        # è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¤„ç†å„ç§ç¼–ç 
        content = None
        encodings = ['utf-8', 'gb2312', 'gbk', 'gb18030']
        
        for encoding in encodings:
            try:
                with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
                    content = f.read()
                break
            except UnicodeDecodeError:
                continue
        
        if not content:
            print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {filepath}")
            return None
        
        # æ¸…ç†æ–‡æœ¬å†…å®¹
        content = content.strip()
        if not content:
            return None
        
        # æå–å…ƒæ•°æ®
        metadata = extract_text_metadata(content, relative_path)
        
        # å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œè¿›è¡Œæ™ºèƒ½æˆªå–
        if len(content) > 50000:
            # ä¿ç•™å‰30000å­—ç¬¦å’Œå5000å­—ç¬¦
            content = content[:30000] + "\n\n...[æ–‡æ¡£è¿‡é•¿ï¼Œå·²æˆªå–ä¸­é—´éƒ¨åˆ†]...\n\n" + content[-5000:]
            metadata['truncated'] = True
        else:
            metadata['truncated'] = False
        
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id = hashlib.md5(relative_path.encode('utf-8')).hexdigest()
        
        # æ„å»ºElasticsearchæ–‡æ¡£
        doc = {
            '_id': doc_id,
            '_source': {
                'title': metadata.get('title', os.path.splitext(metadata['filename'])[0]),
                'chapter': metadata.get('chapter', ''),
                'collection': metadata['collection'],
                'collection_en': metadata['collection_en'],
                'book_category': metadata['book_category'],
                'content': content,
                'filepath': relative_path,
                'filename': metadata['filename'],
                'char_count': metadata['char_count'],
                'line_count': metadata['line_count'],
                'is_classical': metadata['is_classical'],
                'classical_score': metadata['classical_score'],
                'truncated': metadata['truncated'],
                'file_size': os.path.getsize(filepath),
                'indexed_at': datetime.now().isoformat()
            }
        }
        
        return doc
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        return None

def create_chinese_classics_index(es, index_name):
    """åˆ›å»ºä¸“é—¨ç”¨äºä¸­å›½å¤å…¸æ–‡çŒ®çš„ç´¢å¼•"""
    mapping = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "max_result_window": 50000,
            "analysis": {
                "analyzer": {
                    "chinese_text_analyzer": {
                        "type": "standard",
                        "stopwords": ["çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™"]
                    },
                    "classical_chinese_analyzer": {
                        "type": "keyword"
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "title": {
                    "type": "text",
                    "analyzer": "chinese_text_analyzer",
                    "fields": {
                        "keyword": {"type": "keyword"}
                    }
                },
                "chapter": {
                    "type": "text", 
                    "analyzer": "chinese_text_analyzer"
                },
                "collection": {"type": "keyword"},
                "collection_en": {"type": "keyword"},
                "book_category": {"type": "keyword"},
                "content": {
                    "type": "text",
                    "analyzer": "chinese_text_analyzer"
                },
                "filepath": {"type": "keyword"},
                "filename": {"type": "keyword"},
                "char_count": {"type": "integer"},
                "line_count": {"type": "integer"},
                "is_classical": {"type": "boolean"},
                "classical_score": {"type": "integer"},
                "truncated": {"type": "boolean"},
                "file_size": {"type": "long"},
                "indexed_at": {"type": "date"}
            }
        }
    }
    
    try:
        if es.indices.exists(index=index_name):
            print(f"ğŸ“ ç´¢å¼• {index_name} å·²å­˜åœ¨")
            # å¯é€‰ï¼šåˆ é™¤å¹¶é‡å»ºç´¢å¼•
            # es.indices.delete(index=index_name)
            # es.indices.create(index=index_name, body=mapping)
            # print(f"ğŸ”„ é‡å»ºç´¢å¼• {index_name}")
        else:
            es.indices.create(index=index_name, body=mapping)
            print(f"âœ… åˆ›å»ºç´¢å¼• {index_name}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")

def bulk_index_documents(es, documents, index_name):
    """æ‰¹é‡ç´¢å¼•æ–‡æ¡£"""
    try:
        actions = []
        for doc in documents:
            if doc:
                action = {
                    '_index': index_name,
                    '_id': doc['_id'],
                    '_source': doc['_source']
                }
                actions.append(action)
        
        if actions:
            success, failed = bulk(es, actions, request_timeout=120, max_retries=3)
            if failed:
                print(f"âŒ å¤±è´¥ {len(failed)} ä¸ªæ–‡æ¡£")
            return success
        
        return 0
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡ç´¢å¼•å¤±è´¥: {e}")
        return 0

def show_import_stats(es, index_name, data_dir):
    """æ˜¾ç¤ºå¯¼å…¥ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # åŸºæœ¬ç»Ÿè®¡
        stats = es.indices.stats(index=index_name)
        doc_count = stats['indices'][index_name]['total']['docs']['count']
        size_mb = stats['indices'][index_name]['total']['store']['size_in_bytes'] / 1024 / 1024
        
        print(f"\nğŸ“Š å¯¼å…¥å®Œæˆç»Ÿè®¡:")
        print(f"  æ€»æ–‡æ¡£æ•°: {doc_count}")
        print(f"  ç´¢å¼•å¤§å°: {size_mb:.2f} MB")
        
        # æŒ‰è—ç»Ÿè®¡
        search_result = es.search(
            index=index_name,
            body={
                "aggs": {
                    "by_collection": {
                        "terms": {
                            "field": "collection",
                            "size": 20
                        }
                    },
                    "by_classical": {
                        "terms": {
                            "field": "is_classical"
                        }
                    }
                },
                "size": 0
            }
        )
        
        print(f"\nğŸ“š å„è—åˆ†å¸ƒ:")
        for bucket in search_result['aggregations']['by_collection']['buckets']:
            collection = bucket['key']
            count = bucket['doc_count']
            en_name = COLLECTION_MAPPING.get(collection, collection)
            print(f"  {collection} ({en_name}): {count} ä¸ªæ–‡æ¡£")
        
        # å¤æ–‡ç»Ÿè®¡
        classical_stats = search_result['aggregations']['by_classical']['buckets']
        classical_true = next((b['doc_count'] for b in classical_stats if b['key']), 0)
        print(f"\nğŸ“œ å¤å…¸æ–‡çŒ®ç‰¹å¾:")
        print(f"  å¤æ–‡æ–‡æ¡£: {classical_true} ä¸ª")
        print(f"  ç°ä»£æ–‡æ¡£: {doc_count - classical_true} ä¸ª")
        
        # æµ‹è¯•æœç´¢
        print(f"\nğŸ” æœç´¢åŠŸèƒ½æµ‹è¯•:")
        test_queries = ["å²", "é“", "åŒ»", "è¯—"]
        for query in test_queries:
            result = es.search(
                index=index_name,
                body={
                    "query": {"match": {"content": query}},
                    "size": 0
                }
            )
            count = result['hits']['total']['value']
            print(f"  '{query}': {count} ä¸ªç›¸å…³æ–‡æ¡£")
        
    except Exception as e:
        print(f"âŒ ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(description='å¯¼å…¥ä¸­å›½å¤å…¸æ–‡çŒ®åˆ°Elasticsearch')
    parser.add_argument('--dir', default='/home/ubuntu/daizhigev20', help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--index', default='chinese-classics', help='Elasticsearchç´¢å¼•å')
    parser.add_argument('--batch-size', type=int, default=50, help='æ‰¹é‡å¤„ç†å¤§å°')
    parser.add_argument('--dry-run', action='store_true', help='ä»…æ‰«æä¸å¯¼å…¥')
    
    args = parser.parse_args()
    
    # è¿æ¥Elasticsearch
    es = connect_to_elasticsearch()
    if not es:
        return
    
    data_dir = args.dir
    print(f"ğŸ” æ‰«æç›®å½•: {data_dir}")
    
    # æ”¶é›†æ‰€æœ‰txtæ–‡ä»¶
    all_files = []
    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith('.txt'):
                filepath = os.path.join(root, file)
                all_files.append(filepath)
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
    
    # æŒ‰è—åˆ†ç»„æ˜¾ç¤ºç»Ÿè®¡
    collection_stats = {}
    total_size = 0
    for filepath in all_files:
        parts = filepath.split(os.sep)
        collection_found = False
        for part in parts:
            if part in COLLECTION_MAPPING:
                collection = part
                collection_stats[collection] = collection_stats.get(collection, 0) + 1
                collection_found = True
                break
        if not collection_found:
            collection_stats['å…¶ä»–'] = collection_stats.get('å…¶ä»–', 0) + 1
        
        try:
            total_size += os.path.getsize(filepath)
        except:
            pass
    
    print(f"\nğŸ“š å„è—æ–‡ä»¶ç»Ÿè®¡ (æ€»å¤§å°: {total_size/1024/1024:.2f} MB):")
    for collection, count in sorted(collection_stats.items()):
        if collection in COLLECTION_MAPPING:
            en_name = COLLECTION_MAPPING[collection]
            print(f"  {collection} ({en_name}): {count} ä¸ªæ–‡ä»¶")
        else:
            print(f"  {collection}: {count} ä¸ªæ–‡ä»¶")
    
    if args.dry_run:
        print("\nğŸ” å¹²è¿è¡Œå®Œæˆï¼Œæœªè¿›è¡Œå®é™…å¯¼å…¥")
        return
    
    # åˆ›å»ºç´¢å¼•
    create_chinese_classics_index(es, args.index)
    
    # æ‰¹é‡å¤„ç†æ–‡ä»¶
    total_indexed = 0
    batch_docs = []
    
    print(f"\nğŸš€ å¼€å§‹å¯¼å…¥æ•°æ®...")
    
    for i, filepath in enumerate(all_files, 1):
        filename = os.path.basename(filepath)
        
        # æ‰¾åˆ°æ‰€å±çš„è—
        collection = 'æœªçŸ¥'
        for part in filepath.split(os.sep):
            if part in COLLECTION_MAPPING:
                collection = part
                break
        
        if i % 50 == 1 or i <= 10:  # å‰10ä¸ªå’Œæ¯50ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            print(f"ğŸ“ [{i:5d}/{len(all_files)}] å¤„ç†: {collection}/{filename}")
        
        doc = process_text_file(filepath, data_dir)
        if doc:
            batch_docs.append(doc)
        
        # è¾¾åˆ°æ‰¹é‡å¤§å°æˆ–æ˜¯æœ€åä¸€æ‰¹
        if len(batch_docs) >= args.batch_size or i == len(all_files):
            indexed = bulk_index_documents(es, batch_docs, args.index)
            total_indexed += indexed
            batch_docs = []
            
            if i % 200 == 0 or i == len(all_files):  # æ¯200ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡æ€»è¿›åº¦
                print(f"ğŸ“ˆ è¿›åº¦: {i}/{len(all_files)} ({i/len(all_files)*100:.1f}%) - å·²ç´¢å¼•: {total_indexed}")
    
    print(f"\nğŸ‰ å¯¼å…¥å®Œæˆï¼æ€»è®¡ç´¢å¼•: {total_indexed} ä¸ªæ–‡æ¡£")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
    show_import_stats(es, args.index, data_dir)

if __name__ == "__main__":
    main()
