#!/usr/bin/env python3
"""
ä¸­å›½å¤å…¸æ–‡çŒ®æ•°å­—åŒ–èµ„æ–™å¯¼å…¥Elasticsearch
ä¸“é—¨å¤„ç†daizhigev20ç›®å½•ä¸‹çš„ä¼ ç»Ÿå…¸ç±æ•°æ®
"""

import os
import json
import hashlib
import re
from pathlib import Path
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import argparse
from datetime import datetime

# å…¸ç±åˆ†ç±»æ˜ å°„
COLLECTION_MAPPING = {
    'ä½›è—': 'Buddhist Texts',
    'å„’è—': 'Confucian Classics', 
    'åŒ»è—': 'Medical Texts',
    'å²è—': 'Historical Records',
    'å­è—': 'Masters Literature',
    'æ˜“è—': 'I Ching Studies',
    'è‰ºè—': 'Arts & Crafts',
    'è¯—è—': 'Poetry Collection',
    'é“è—': 'Taoist Texts',
    'é›†è—': 'Collected Works'
}

def connect_to_elasticsearch():
    """è¿æ¥åˆ°Elasticsearch"""
    try:
        es = Elasticsearch([{'host': 'localhost', 'port': 9200}])
        if es.ping():
            print("âœ… æˆåŠŸè¿æ¥åˆ°Elasticsearch")
            cluster_info = es.info()
            print(f"é›†ç¾¤ä¿¡æ¯: {cluster_info['cluster_name']} - ç‰ˆæœ¬: {cluster_info['version']['number']}")
            return es
        else:
            print("âŒ æ— æ³•è¿æ¥åˆ°Elasticsearch")
            return None
    except Exception as e:
        print(f"âŒ è¿æ¥é”™è¯¯: {e}")
        return None

def safe_read_file(filepath):
    """å®‰å…¨è¯»å–æ–‡ä»¶ï¼Œå¤„ç†å„ç§ç¼–ç """
    encodings = ['utf-8', 'gb2312', 'gbk', 'gb18030', 'big5', 'cp936']
    
    for encoding in encodings:
        try:
            with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
                content = f.read()
            if content.strip():  # ç¡®ä¿ä¸æ˜¯ç©ºæ–‡ä»¶
                return content
        except (UnicodeDecodeError, IOError):
            continue
    
    return None

def extract_text_metadata(content, filepath):
    """ä»æ–‡æœ¬å†…å®¹ä¸­æå–å…ƒæ•°æ® - ä¿®å¤ç‰ˆ"""
    metadata = {}
    
    try:
        # å®‰å…¨è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        filename = os.path.basename(filepath)
        path_parts = filepath.split(os.sep)
        
        # åˆå§‹åŒ–é»˜è®¤å€¼
        metadata['filename'] = filename
        metadata['collection'] = 'æœªåˆ†ç±»'
        metadata['collection_en'] = 'Uncategorized'
        metadata['book_category'] = 'å…¶ä»–'
        
        # ä»è·¯å¾„ä¸­æå–è—å’Œåˆ†ç±»ä¿¡æ¯
        for i, part in enumerate(path_parts):
            if part in COLLECTION_MAPPING:
                metadata['collection'] = part
                metadata['collection_en'] = COLLECTION_MAPPING[part]
                
                # å°è¯•è·å–ä¹¦ç±åˆ†ç±»ï¼ˆè—ä¸‹é¢çš„å­ç›®å½•ï¼‰
                if i + 1 < len(path_parts) - 1:  # æœ‰å­ç›®å½•ä¸”ä¸æ˜¯æœ€åçš„æ–‡ä»¶å
                    metadata['book_category'] = path_parts[i + 1]
                break
        
        # å°è¯•ä»æ–‡ä»¶å†…å®¹æå–æ ‡é¢˜
        if content:
            lines = content.split('\n')[:30]  # å¢åŠ æ£€æŸ¥è¡Œæ•°
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æ ‡é¢˜çš„è¡Œ
                if len(line) > 100 or any(char in line for char in ['http', 'www', '@']):
                    continue
                    
                # å¯»æ‰¾å¯èƒ½çš„æ ‡é¢˜
                if len(line) <= 50:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
                    if any(char in line for char in ['å·', 'ç¯‡', 'ç« ', 'ç¬¬', 'åº', 'è·‹', 'å‰è¨€']):
                        if 'chapter' not in metadata:
                            metadata['chapter'] = line
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¹¦å
                    elif len(line) <= 30 and not any(char in line for char in ['ã€‚', 'ï¼Œ', 'ï¼Ÿ', 'ï¼']):
                        if 'title' not in metadata:
                            metadata['title'] = line
                        break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨æ–‡ä»¶åï¼ˆå»æ‰.txtï¼‰
        if 'title' not in metadata:
            title = os.path.splitext(filename)[0]
            metadata['title'] = title
            
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç« èŠ‚ï¼Œè®¾ä¸ºç©º
        if 'chapter' not in metadata:
            metadata['chapter'] = ''
        
        # æ–‡æœ¬ç»Ÿè®¡
        if content:
            metadata['char_count'] = len(content)
            metadata['line_count'] = len(content.split('\n'))
            
            # å¤æ–‡ç‰¹å¾æ£€æµ‹
            classical_indicators = ['æ›°', 'è€…', 'ä¹Ÿ', 'çŸ£', 'ç„‰', 'ä¹', 'å“‰', 'è€¶', 'ä¹‹', 'å…¶', 'è€Œ']
            classical_score = sum(content.count(char) for char in classical_indicators)
            metadata['classical_score'] = classical_score
            metadata['is_classical'] = classical_score > 20
        else:
            metadata['char_count'] = 0
            metadata['line_count'] = 0
            metadata['classical_score'] = 0
            metadata['is_classical'] = False
        
        return metadata
        
    except Exception as e:
        print(f"âš ï¸  å…ƒæ•°æ®æå–è­¦å‘Š {filepath}: {e}")
        # è¿”å›åŸºæœ¬å…ƒæ•°æ®
        return {
            'filename': os.path.basename(filepath),
            'collection': 'æœªåˆ†ç±»',
            'collection_en': 'Uncategorized',
            'book_category': 'å…¶ä»–',
            'title': os.path.splitext(os.path.basename(filepath))[0],
            'chapter': '',
            'char_count': 0,
            'line_count': 0,
            'classical_score': 0,
            'is_classical': False
        }

def process_text_file(filepath, base_dir):
    """å¤„ç†å•ä¸ªå¤å…¸æ–‡çŒ®æ–‡ä»¶ - å¢å¼ºå®¹é”™ç‰ˆ"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(filepath):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return None
            
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(filepath)
        if file_size == 0:
            print(f"âš ï¸  è·³è¿‡ç©ºæ–‡ä»¶: {filepath}")
            return None
            
        # è·å–ç›¸å¯¹è·¯å¾„
        try:
            relative_path = os.path.relpath(filepath, base_dir)
        except Exception as e:
            print(f"âš ï¸  è·¯å¾„å¤„ç†é”™è¯¯ {filepath}: {e}")
            relative_path = filepath
        
        # å®‰å…¨è¯»å–æ–‡ä»¶å†…å®¹
        content = safe_read_file(filepath)
        if not content:
            print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: {filepath}")
            return None
        
        # æ¸…ç†å’ŒéªŒè¯å†…å®¹
        content = content.strip()
        if len(content) < 10:  # å†…å®¹å¤ªå°‘ï¼Œå¯èƒ½ä¸æ˜¯æœ‰æ•ˆæ–‡æ¡£
            print(f"âš ï¸  è·³è¿‡å†…å®¹è¿‡å°‘çš„æ–‡ä»¶: {filepath}")
            return None
        
        # æå–å…ƒæ•°æ®
        metadata = extract_text_metadata(content, relative_path)
        
        # å¤„ç†è¿‡é•¿çš„å†…å®¹
        original_length = len(content)
        if original_length > 100000:  # 100KBä»¥ä¸Šæˆªå–
            # ä¿ç•™å‰60000å­—ç¬¦å’Œå20000å­—ç¬¦
            content = content[:60000] + f"\n\n...[æ–‡æ¡£è¿‡é•¿({original_length}å­—ç¬¦)ï¼Œå·²æˆªå–ä¸­é—´éƒ¨åˆ†]...\n\n" + content[-20000:]
            metadata['truncated'] = True
            metadata['original_char_count'] = original_length
        else:
            metadata['truncated'] = False
            metadata['original_char_count'] = original_length
        
        # ç”Ÿæˆæ–‡æ¡£IDï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ç¡®ä¿ä¸€è‡´æ€§ï¼‰
        doc_id = hashlib.md5(relative_path.encode('utf-8')).hexdigest()
        
        # æ„å»ºElasticsearchæ–‡æ¡£
        doc = {
            '_id': doc_id,
            '_source': {
                'title': metadata['title'],
                'chapter': metadata['chapter'],
                'collection': metadata['collection'],
                'collection_en': metadata['collection_en'],
                'book_category': metadata['book_category'],
                'content': content,
                'filepath': relative_path,
                'filename': metadata['filename'],
                'char_count': metadata['char_count'],
                'line_count': metadata['line_count'],
                'is_classical': metadata['is_classical'],
                'classical_score': metadata['classical_score'],
                'truncated': metadata['truncated'],
                'original_char_count': metadata['original_char_count'],
                'file_size': file_size,
                'indexed_at': datetime.now().isoformat()
            }
        }
        
        return doc
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        return None

def create_chinese_classics_index(es, index_name):
    """åˆ›å»ºä¸“é—¨ç”¨äºä¸­å›½å¤å…¸æ–‡çŒ®çš„ç´¢å¼•ï¼ˆä½¿ç”¨IKåˆ†è¯å™¨ï¼‰"""
    mapping = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "max_result_window": 50000,
            "analysis": {
                "analyzer": {
                    "ik_chinese_analyzer": {
                        "type": "ik_max_word"
                    },
                    "ik_chinese_search_analyzer": {
                        "type": "ik_smart"
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "title": {
                    "type": "text",
                    "analyzer": "ik_chinese_analyzer",
                    "search_analyzer": "ik_chinese_search_analyzer",
                    "fields": {
                        "keyword": {"type": "keyword"}
                    }
                },
                "chapter": {
                    "type": "text", 
                    "analyzer": "ik_chinese_analyzer",
                    "search_analyzer": "ik_chinese_search_analyzer"
                },
                "collection": {"type": "keyword"},
                "collection_en": {"type": "keyword"},
                "book_category": {"type": "keyword"},
                "content": {
                    "type": "text",
                    "analyzer": "ik_chinese_analyzer",
                    "search_analyzer": "ik_chinese_search_analyzer"
                },
                "filepath": {"type": "keyword"},
                "filename": {"type": "keyword"},
                "char_count": {"type": "integer"},
                "line_count": {"type": "integer"},
                "is_classical": {"type": "boolean"},
                "classical_score": {"type": "integer"},
                "truncated": {"type": "boolean"},
                "original_char_count": {"type": "integer"},
                "file_size": {"type": "long"},
                "indexed_at": {"type": "date"}
            }
        }
    }
    
    try:
        if es.indices.exists(index=index_name):
            print(f"ğŸ“ ç´¢å¼• {index_name} å·²å­˜åœ¨")
        else:
            es.indices.create(index=index_name, body=mapping)
            print(f"âœ… åˆ›å»ºç´¢å¼• {index_name}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        return False
    return True

def bulk_index_documents(es, documents, index_name):
    """æ‰¹é‡ç´¢å¼•æ–‡æ¡£"""
    if not documents:
        return 0
        
    try:
        actions = []
        for doc in documents:
            if doc and '_id' in doc and '_source' in doc:
                action = {
                    '_index': index_name,
                    '_id': doc['_id'],
                    '_source': doc['_source']
                }
                actions.append(action)
        
        if actions:
            success, failed = bulk(es, actions, request_timeout=180, max_retries=3, chunk_size=100)
            if failed:
                print(f"âš ï¸  æ‰¹é‡ç´¢å¼•éƒ¨åˆ†å¤±è´¥: æˆåŠŸ{success}, å¤±è´¥{len(failed)}")
                # æ‰“å°å‰å‡ ä¸ªå¤±è´¥çš„è¯¦æƒ…
                for i, fail in enumerate(failed[:3]):
                    print(f"    å¤±è´¥{i+1}: {fail}")
            return success
        
        return 0
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡ç´¢å¼•å¤±è´¥: {e}")
        return 0

def show_import_stats(es, index_name, data_dir):
    """æ˜¾ç¤ºå¯¼å…¥ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # åŸºæœ¬ç»Ÿè®¡
        stats = es.indices.stats(index=index_name)
        doc_count = stats['indices'][index_name]['total']['docs']['count']
        size_mb = stats['indices'][index_name]['total']['store']['size_in_bytes'] / 1024 / 1024
        
        print(f"\nğŸ“Š å¯¼å…¥å®Œæˆç»Ÿè®¡:")
        print(f"  æ€»æ–‡æ¡£æ•°: {doc_count:,}")
        print(f"  ç´¢å¼•å¤§å°: {size_mb:.2f} MB")
        
        # æŒ‰è—ç»Ÿè®¡
        search_result = es.search(
            index=index_name,
            body={
                "aggs": {
                    "by_collection": {
                        "terms": {
                            "field": "collection",
                            "size": 20
                        }
                    },
                    "by_classical": {
                        "terms": {
                            "field": "is_classical"
                        }
                    }
                },
                "size": 0
            }
        )
        
        print(f"\nğŸ“š å„è—åˆ†å¸ƒ:")
        for bucket in search_result['aggregations']['by_collection']['buckets']:
            collection = bucket['key']
            count = bucket['doc_count']
            en_name = COLLECTION_MAPPING.get(collection, collection)
            print(f"  {collection} ({en_name}): {count:,} ä¸ªæ–‡æ¡£")
        
        # å¤æ–‡ç»Ÿè®¡
        classical_stats = search_result['aggregations']['by_classical']['buckets']
        classical_true = next((b['doc_count'] for b in classical_stats if b['key']), 0)
        print(f"\nğŸ“œ å¤å…¸æ–‡çŒ®ç‰¹å¾:")
        print(f"  å¤æ–‡æ–‡æ¡£: {classical_true:,} ä¸ª")
        print(f"  ç°ä»£æ–‡æ¡£: {doc_count - classical_true:,} ä¸ª")
        
        # æµ‹è¯•æœç´¢
        print(f"\nğŸ” æœç´¢åŠŸèƒ½æµ‹è¯•:")
        test_queries = ["å²", "é“", "åŒ»", "è¯—", "ç»"]
        for query in test_queries:
            result = es.search(
                index=index_name,
                body={
                    "query": {"match": {"content": query}},
                    "size": 0
                }
            )
            count = result['hits']['total']['value']
            print(f"  '{query}': {count:,} ä¸ªç›¸å…³æ–‡æ¡£")
        
    except Exception as e:
        print(f"âŒ ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(description='å¯¼å…¥ä¸­å›½å¤å…¸æ–‡çŒ®åˆ°Elasticsearch')
    parser.add_argument('--dir', default='/home/ubuntu/daizhigev20', help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--index', default='chinese-classics', help='Elasticsearchç´¢å¼•å')
    parser.add_argument('--batch-size', type=int, default=30, help='æ‰¹é‡å¤„ç†å¤§å°')
    parser.add_argument('--dry-run', action='store_true', help='ä»…æ‰«æä¸å¯¼å…¥')
    parser.add_argument('--skip-existing', action='store_true', help='è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ ä¸­å›½å¤å…¸æ–‡çŒ®å¯¼å…¥å·¥å…· - å¢å¼ºç‰ˆ")
    print("=" * 60)
    
    # è¿æ¥Elasticsearch
    es = connect_to_elasticsearch()
    if not es:
        return
    
    data_dir = args.dir
    print(f"ğŸ” æ‰«æç›®å½•: {data_dir}")
    
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # æ”¶é›†æ‰€æœ‰txtæ–‡ä»¶
    print("ğŸ“ æ”¶é›†æ–‡ä»¶åˆ—è¡¨...")
    all_files = []
    skipped_files = []
    
    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith('.txt'):
                filepath = os.path.join(root, file)
                try:
                    if os.path.getsize(filepath) > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                        all_files.append(filepath)
                    else:
                        skipped_files.append(filepath)
                except OSError:
                    skipped_files.append(filepath)
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(all_files):,} ä¸ªæœ‰æ•ˆæ–‡æœ¬æ–‡ä»¶")
    if skipped_files:
        print(f"âš ï¸  è·³è¿‡ {len(skipped_files)} ä¸ªç©ºæ–‡ä»¶æˆ–æ— æ³•è®¿é—®çš„æ–‡ä»¶")
    
    # æŒ‰è—åˆ†ç»„æ˜¾ç¤ºç»Ÿè®¡
    collection_stats = {}
    total_size = 0
    for filepath in all_files:
        collection_found = False
        for part in filepath.split(os.sep):
            if part in COLLECTION_MAPPING:
                collection = part
                collection_stats[collection] = collection_stats.get(collection, 0) + 1
                collection_found = True
                break
        if not collection_found:
            collection_stats['æœªåˆ†ç±»'] = collection_stats.get('æœªåˆ†ç±»', 0) + 1
        
        try:
            total_size += os.path.getsize(filepath)
        except:
            pass
    
    print(f"\nğŸ“š å„è—æ–‡ä»¶ç»Ÿè®¡ (æ€»å¤§å°: {total_size/1024/1024:.2f} MB):")
    for collection, count in sorted(collection_stats.items()):
        if collection in COLLECTION_MAPPING:
            en_name = COLLECTION_MAPPING[collection]
            print(f"  {collection} ({en_name}): {count:,} ä¸ªæ–‡ä»¶")
        else:
            print(f"  {collection}: {count:,} ä¸ªæ–‡ä»¶")
    
    if args.dry_run:
        print("\nğŸ” å¹²è¿è¡Œå®Œæˆï¼Œæœªè¿›è¡Œå®é™…å¯¼å…¥")
        return
    
    # åˆ›å»ºç´¢å¼•
    print(f"\nğŸ—ï¸  å‡†å¤‡ç´¢å¼•: {args.index}")
    if not create_chinese_classics_index(es, args.index):
        return
    
    # æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡æ¡£ï¼ˆå¦‚æœå¯ç”¨è·³è¿‡é€‰é¡¹ï¼‰
    existing_ids = set()
    if args.skip_existing:
        print("ğŸ” æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡æ¡£...")
        try:
            result = es.search(
                index=args.index,
                body={"query": {"match_all": {}}, "_source": False},
                size=10000,
                scroll='5m'
            )
            
            while result['hits']['hits']:
                for hit in result['hits']['hits']:
                    existing_ids.add(hit['_id'])
                
                if result.get('_scroll_id'):
                    result = es.scroll(scroll_id=result['_scroll_id'], scroll='5m')
                else:
                    break
                    
            print(f"ğŸ“ æ‰¾åˆ° {len(existing_ids)} ä¸ªå·²å­˜åœ¨çš„æ–‡æ¡£")
        except Exception as e:
            print(f"âš ï¸  æ£€æŸ¥å·²å­˜åœ¨æ–‡æ¡£å¤±è´¥: {e}")
    
    # æ‰¹é‡å¤„ç†æ–‡ä»¶
    total_indexed = 0
    total_skipped = 0
    total_failed = 0
    batch_docs = []
    
    print(f"\nğŸš€ å¼€å§‹å¯¼å…¥æ•°æ®...")
    print(f"æ‰¹é‡å¤§å°: {args.batch_size}")
    
    for i, filepath in enumerate(all_files, 1):
        filename = os.path.basename(filepath)
        
        # æ˜¾ç¤ºè¿›åº¦
        if i % 100 == 1 or i <= 10:
            print(f"ğŸ“ [{i:5d}/{len(all_files)}] å¤„ç†: {filename}")
        
        # ç”Ÿæˆæ–‡æ¡£IDæ£€æŸ¥æ˜¯å¦è·³è¿‡
        relative_path = os.path.relpath(filepath, data_dir)
        doc_id = hashlib.md5(relative_path.encode('utf-8')).hexdigest()
        
        if args.skip_existing and doc_id in existing_ids:
            total_skipped += 1
            continue
        
        # å¤„ç†æ–‡ä»¶
        doc = process_text_file(filepath, data_dir)
        if doc:
            batch_docs.append(doc)
        else:
            total_failed += 1
        
        # è¾¾åˆ°æ‰¹é‡å¤§å°æˆ–æ˜¯æœ€åä¸€æ‰¹
        if len(batch_docs) >= args.batch_size or i == len(all_files):
            if batch_docs:
                indexed = bulk_index_documents(es, batch_docs, args.index)
                total_indexed += indexed
                batch_docs = []
            
            # æ˜¾ç¤ºè¿›åº¦
            if i % 500 == 0 or i == len(all_files):
                print(f"ğŸ“ˆ è¿›åº¦: {i:,}/{len(all_files):,} ({i/len(all_files)*100:.1f}%)")
                print(f"   å·²ç´¢å¼•: {total_indexed:,}, è·³è¿‡: {total_skipped:,}, å¤±è´¥: {total_failed:,}")
    
    print(f"\nğŸ‰ å¯¼å…¥å®Œæˆ!")
    print(f"ğŸ“Š æ€»è®¡: ç´¢å¼•{total_indexed:,}, è·³è¿‡{total_skipped:,}, å¤±è´¥{total_failed:,}")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
    if total_indexed > 0:
        show_import_stats(es, args.index, data_dir)
    
    print("\nâœ¨ å¯¼å…¥ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()
